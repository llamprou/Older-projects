# Older-projects
I include here two of the initial projects I completed: 

1) A custom-made multilayer convolutional-pooling network with backpropagation built from scratch which I trained on the FashionMNIST database used in a Pytorch tutorial. This network is super simple-minded and not efficient at all for a number of reasons. Primarily: I didn't vectorize the batch and I chose a very costly (yet mathematically natural) way to apply the convolutional and pooling filters. This project was intended to be a sightly less trivial exercise on the mechanics of backpropagation.

2) An LSTM network for character-level text generation. This was intended to expose me to RNNs in general and the slightly more involved LSTM architecture. I simply trained this on a short text passage I wrote personally, as a basic test test that the model works. I did not perfrom any in-depth analysis or experimentation with it because I wanted to focus on the transformer.

These peojects are not cleaned up and I did not revisit them again after becoming more competent with coding. I include them here to provide a larger context of my ML preparation.
