{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Character level text generator with a single LSTM cell. My goal was to make sure that it works and that it learns, while at the same time training myself on a few more software engineering tricks. It appears to be able to learn phrases."
      ],
      "metadata": {
        "id": "u9VTlNaCipC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "Ap6pKv7lzX5y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary class with torch.tensor encoding and decoding methods\n",
        "class My_dictionary(object):\n",
        "  def __init__(self, text):\n",
        "    self.alphabet = list(set(text))\n",
        "    self.alphabet_dictionary = {}\n",
        "    for k in range(len(self.alphabet)):\n",
        "      self.alphabet_dictionary[self.alphabet[k]] = torch.zeros(len(self.alphabet)).scatter_(0, torch.tensor(k), 1 )\n",
        "\n",
        "\n",
        "\n",
        "  def text_encoding(self, text):\n",
        "    data=[]\n",
        "    for char in text:\n",
        "     data.append(self.alphabet_dictionary[char])  \n",
        "    input_tensor = torch.stack(tuple(data), dim =0)\n",
        "    return input_tensor\n",
        "\n",
        "\n",
        "\n",
        "  def text_decoding(self, input):\n",
        "    text_output= \"\"\n",
        "    input_det = input.detach()    \n",
        "    for k in range(len(input_det)):\n",
        "      inp_list = input_det[k].tolist()\n",
        "      probability = [elem/sum(inp_list) for elem in inp_list] #This renormalization of probability is needed due to a truncation error in going from torch.tensors to lists. It is such a small difference that is irrelevant.\n",
        "      sampled_char = np.random.choice(list(self.alphabet_dictionary.keys()), p= probability)\n",
        "      text_output += sampled_char \n",
        "    return text_output\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Building an iterator over minibatches of inputed text after tensor encoding\n",
        "class Dataloader_iter(object):\n",
        "  def __init__(self, text, dictionary, batch_size, shuffle_batch=False):\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle_batch = shuffle_batch\n",
        "    self.length= len(text)\n",
        "    self.text = text\n",
        "\n",
        "    self.dictionary = dictionary\n",
        "    self.encoding = self.dictionary.text_encoding(text)\n",
        "\n",
        "  \n",
        "\n",
        "  def get_batches(self):\n",
        "    bs= self.batch_size\n",
        "    input, expectation = self.encoding[:-1], self.encoding[1:]\n",
        "    minibatches = [(input[k*bs:(k+1)*bs], expectation[k*bs : (k+1)*bs]) for k in range(int(self.length/bs))]\n",
        "\n",
        "    if (self.shuffle_batch):\n",
        "      random.shuffle(minibatches)\n",
        "    \n",
        "    self.minibatches = minibatches\n",
        "  \n",
        "\n",
        "  def __iter__(self):\n",
        "    self.indx =0\n",
        "    self.get_batches()\n",
        "    return self\n",
        "\n",
        "\n",
        "  def __next__(self):\n",
        "    if (self.indx >= int(self.length/self.batch_size)):\n",
        "      raise StopIteration\n",
        "    self.indx +=1\n",
        "    return self.minibatches[self.indx -1]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9akSoQfZIRch"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM cell\n",
        "\n",
        "class Lstm_cell(nn.Module):\n",
        "  def __init__(self, cell_size, inp_size):\n",
        "    super().__init__()\n",
        "    self.cell_size = cell_size\n",
        "    self.inp_size = inp_size\n",
        "\n",
        "    self.memory_state= torch.zeros(cell_size)\n",
        "    self.cell= torch.zeros(cell_size)\n",
        "\n",
        "    self.fgate = nn.Linear(cell_size + inp_size, cell_size, bias=True) #forget gate\n",
        "    self.ingate = nn.Linear(cell_size + inp_size, cell_size, bias=True) #in gate\n",
        "    self.intomem = nn.Linear(cell_size + inp_size, cell_size, bias=True)  #input layer\n",
        "\n",
        "    self.outgate = nn.Linear(cell_size + inp_size, cell_size, bias=True) #out gate\n",
        "    self.celltoout = nn.Linear(cell_size, inp_size, bias=False) #output layer\n",
        "\n",
        "    self.activation = nn.Sigmoid() #I seem to be getting better information propagation with the sigmoid as compared to relu\n",
        "    self.amplifier= torch.tensor(2.) #rescales some activations below\n",
        "\n",
        "\n",
        "  #reseting memory and cell state\n",
        "  def forget_everything(self):\n",
        "    self.memory_state =torch.zeros(self.cell_size)\n",
        "    self.cell =torch.zeros(self.cell_size)\n",
        "\n",
        "  \n",
        "  def tanh_act(self,x):\n",
        "    return torch.tanh(x)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, inp):\n",
        "    #removing input+cell state gradient for truncated backpropagation algorithm\n",
        "    cell_trunc = self.cell.detach()\n",
        "    inp_trunc = inp.detach()\n",
        "    #total cell input\n",
        "    y = torch.cat((cell_trunc, inp_trunc), dim = 0)\n",
        "    \n",
        "\n",
        "    #in-gate activation\n",
        "    write_control= self.activation(self.ingate(y))\n",
        "    #write-in-memory activation\n",
        "    write_input = self.amplifier*self.tanh_act(self.intomem(y))\n",
        "    \n",
        "\n",
        "    #forget-and-write memory state update\n",
        "    regulator= 1. #adding the option of renormalizing the memory state to prevent it from saturating the tanh activation\n",
        "    self.memory_state =  ((write_control * write_input) + self.memory_state*self.activation(self.fgate(y)))/regulator \n",
        "\n",
        "\n",
        "    #out-gate activation\n",
        "    read_control = self.activation(self.outgate(y))\n",
        "    #read-from-memory activation\n",
        "    read_output= self.tanh_act(self.memory_state) #renormalization of output in order to prevent softmax blow-ups --value chosen by trial and error\n",
        "\n",
        "    #total cell output\n",
        "    out = read_control * read_output*self.amplifier \n",
        "    self.cell = out\n",
        "    out = self.celltoout(self.cell)\n",
        "    \n",
        "    return out"
      ],
      "metadata": {
        "id": "0Wumi3lrzZtC"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, cells, activation= nn.Softmax(dim=1)):\n",
        "    super().__init__()\n",
        "    self.cells = cells\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "    self.softmax= activation\n",
        "  \n",
        "\n",
        "  #forward method\n",
        "  def forward(self, text, activate=True, forget= True):\n",
        "    if forget: self.cells.forget_everything()\n",
        "    output = (self.cells.forward(text[0])).unsqueeze(0)\n",
        "\n",
        "    for elem in range(1,len(text)):\n",
        "      out = self.cells.forward(text[elem])\n",
        "      output = torch.cat((output, out.unsqueeze(0)), dim = 0)   \n",
        "    if activate: output= self.softmax(output)\n",
        "    return output\n",
        "\n",
        "  \n",
        "\n",
        "  #Evaluate progress\n",
        "  def evaluate(self, test_batch):\n",
        "    self.cells.forget_everything()\n",
        "    prediction = self.forward(test_batch[0]).detach()\n",
        "    accuracy= (((torch.argmax(test_batch[1],dim=1)==torch.argmax(prediction, dim=1))).sum())/len(test_batch[0])\n",
        "    return accuracy\n",
        "  \n",
        "\n",
        "  #generate text --method 1: Picking most likely element at every step\n",
        "  def generate_max(self, start, length, forget=True):\n",
        "    if forget: self.cells.forget_everything()\n",
        "    idx = torch.argmax(self.cells.forward(start).detach())\n",
        "    out = torch.zeros_like(start)\n",
        "    out[idx] =torch.tensor(1.)\n",
        "    output=out.unsqueeze(0)\n",
        "\n",
        "    for elem in range(length-1):\n",
        "      idx = torch.argmax(self.cells.forward(out))\n",
        "      out = torch.zeros_like(start)\n",
        "      out[idx] =torch.tensor(1.)\n",
        "      output = torch.cat((output, out.unsqueeze(0)), dim = 0)  \n",
        "    output = torch.cat((start.unsqueeze(0), output), dim = 0) \n",
        "    return output\n",
        "\n",
        "\n",
        "  #generate text --method 2: Using output probabilities for random character sampling at every step\n",
        "  def generate_prob(self, start, length, forget=True):\n",
        "    if forget: self.cells.forget_everything()\n",
        "\n",
        "    out = start\n",
        "    output = start.unsqueeze(0) \n",
        "\n",
        "    for k in range(length):\n",
        "      out = self.cells.forward(out)\n",
        "      out= self.softmax(out.unsqueeze(0))\n",
        "\n",
        "      probability =(out.squeeze()).tolist()\n",
        "      probability = [elem/sum(probability) for elem in probability]\n",
        "      idx = np.random.choice(len(start), p= probability)\n",
        "      out =torch.zeros_like(start)\n",
        "      out[idx] =1\n",
        "      \n",
        "      output = torch.cat((output, out.unsqueeze(0)), dim = 0) \n",
        "    return output\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "8mbfZVpuNop1"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constructing the model, data\n",
        "\n",
        "def get_data(text, bs):\n",
        "  dictionary = My_dictionary(text)\n",
        "  train_dataloader = Dataloader_iter(text, My_dictionary(text), batch_size=bs, shuffle_batch=True)\n",
        "  return dictionary, train_dataloader\n",
        "\n",
        "\n",
        "\n",
        "def get_model(cell_size, inp_size, lr):\n",
        "  model = Network(Lstm_cell(cell_size,inp_size))\n",
        "  opt = optim.SGD(model.cells.parameters(), lr)\n",
        "  return model, opt\n"
      ],
      "metadata": {
        "id": "YONnk2HDLomK"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text generator\n",
        "def generate_text(model, dictionary, start, length, forget=True):\n",
        "  start_vec = dictionary.alphabet_dictionary[start]\n",
        "  text_instance_encoded = model.generate_prob(start_vec, length, forget)\n",
        "  text_instance = dictionary.text_decoding(text_instance_encoded)\n",
        "  print(text_instance)"
      ],
      "metadata": {
        "id": "6zk-KXF2yI2Y"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training function\n",
        "def fit(epochs, model, opt, dictionary, training_dataloader, test_data=None, length=None, start=None, forget=True):\n",
        "  for epoch in range(epochs):\n",
        "    train_data_iter = iter(training_dataloader)\n",
        "    for input_batch, expect_batch in train_data_iter:\n",
        "      #training method for text minibatch\n",
        "      total_loss =0\n",
        "      #reset memory and cell state\n",
        "      model.cells.forget_everything()\n",
        "      #forward pass\n",
        "      out = model.forward(input_batch, activate=False)\n",
        "      #backward pass\n",
        "      loss = model.loss(out , torch.argmax(expect_batch, dim=1))\n",
        "      total_loss += loss\n",
        "      loss.backward()\n",
        "      #print(f\"{[(torch.min(p.grad), torch.max(p.grad)) for p in model.parameters() ]}\")\n",
        "\n",
        "      #update text sequence minibatch    \n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "    #printing out some data to make sure activations are not blowing up during training\n",
        "    print(f\"memory: {(torch.min(model.cells.memory_state), torch.max(model.cells.memory_state))} \\n softmax: {model.softmax(model.cells.cell.unsqueeze(0))}\")\n",
        "\n",
        "    \n",
        "    #compute loss on test data\n",
        "    if test_data:\n",
        "      test_data_iter = iter(test_data)\n",
        "      accuracy=0\n",
        "      with torch.no_grad():\n",
        "        for exp, pred in test_data_iter:\n",
        "          accuracy += model.evaluate((exp, pred))\n",
        "      print(f\"Epoch {epoch} achieved accuracy {accuracy/len(test_data_iter)}\")\n",
        "    \n",
        "    #generate new text\n",
        "    if (length is not None and start is not None):\n",
        "      with torch.no_grad():\n",
        "        generate_text(model, dictionary, start, length, forget)\n"
      ],
      "metadata": {
        "id": "dKBYl0MTyARk"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example =\"i think i may have actually managed to make this thing train! great news. now i can go on to study the transformer at last. \"\n"
      ],
      "metadata": {
        "id": "5GD94OrMjlTG"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary, train_dataloader= get_data(example*500, 20)"
      ],
      "metadata": {
        "id": "Bsklug3rNEnc"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt =get_model(2*len(dictionary.alphabet), len(dictionary.alphabet), 0.1)"
      ],
      "metadata": {
        "id": "LZQjbqh_rvTf"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, dictionary, \"w\", 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncs9N2D1PPGu",
        "outputId": "a83c99ca-b403-4982-a137-a91cd336287e"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w.moygmnakndyw.ftaoes!lovitvawg!mkmtrwytoitakdfled!dagknkfdgya.dtylfdkhfghhsruhaavrfifg.tkcnt!tyv!gdg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cells.forget_everything()\n",
        "for char in dictionary.alphabet: generate_text(model, dictionary, char, 100) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvUhn8Q9pBMU",
        "outputId": "3bd8e087-68df-49d7-bb19-cd11ca944694"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ve actually managed to make this thing train! grean nos tudy to in thi k yan go oran gormer at last. \n",
            "o thin think great new lcan so mau!y train! great new sran grean new i can go on to study the transfo\n",
            " thing train! great new in tuaink gret at last. i think i may have actually managed to make this thin\n",
            "this thing train! great new sonmar at last. i think i may have actually managed to make this thing tr\n",
            "ke trans. trat news. i this thing train! great news. now i can go on to study the transformer at last\n",
            "ged to make this thing train! great new tcin think train! great news. now i can go on to study the tr\n",
            "news. now i can go on to  thdy the transformer at last. i think i may have actually managed to make t\n",
            "former at last. i think i may have actually managed to make this this this thinggrratraat new traistf\n",
            "w i can go on to study the transformer at last. i think i may have actually managed to make this thin\n",
            "ink i may have actually managed to make this thing train! great news. now i can go on to study the tr\n",
            "eat news. now i can go on to study the transformer at last. i think i may have actually managed to ma\n",
            "dy the transformer at last. i think i may have actually managed to make this thing train! greai  ctua\n",
            "ake this thing train! grean go study the transformer at last. i think i may have actually managed to \n",
            "st. i think i may have actually managed to make this thing train! grve trasformer at last. i think i \n",
            "y have actually managedr t toss. now i can go on to study the transformer at last. i think i may have\n",
            "uanakeet atully smans triin! great news. now i can go on to study the transformer at last. i think i \n",
            ". now i can go on to study the transformer at last. i think i may gave actually managed to make this \n",
            "can go on to study the transformer at llly managed transftrain! gr at ness. now to ii ky nane dav nan\n",
            "managed to make this thing train! great news. now train! great newsw now i can go on to study the tra\n",
            "rain! great news. now i can go on to study the transformer at last. i think i may have actually manag\n",
            "! great news. now i can go on to study the transformer at last. i think i may have actually managed t\n",
            "he transformer at last. i think i may have actually managed to make this thing train! great news. now\n",
            "lly managed to make this thing train! great newst.nnw i can go on to study the transformer at last. i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = dictionary.text_encoding(example)\n",
        "text_output= model.forward(inp)\n",
        "dictionary.text_decoding(text_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X-wxNqu71ziD",
        "outputId": "4ff1de35-d164-4162-c038-a8a0bbc754bd"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ckd!g ggahks  ihromfwofegoss rckatudttohmddm wf a!mf!wdcrdwgmsmdt!vwwckng owuy yvmrkms.rmkrutld!mnfdgakags ahoeshs!yg! gghrs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cells.forget_everything()\n",
        "fit(100,model,opt,dictionary,train_dataloader,test_data =None, length= 100, start= \"h\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "zTE8-BtVvCvB",
        "outputId": "8d6b751c-46b4-4a9e-9ea6-08519704a118"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory: (tensor(-2.5514, grad_fn=<MinBackward1>), tensor(2.4136, grad_fn=<MaxBackward1>)) \n",
            " softmax: tensor([[0.0032, 0.0021, 0.0258, 0.0147, 0.0017, 0.0408, 0.0035, 0.0039, 0.0636,\n",
            "         0.0238, 0.0043, 0.0269, 0.0040, 0.0297, 0.0024, 0.0713, 0.0428, 0.0229,\n",
            "         0.0062, 0.0020, 0.0037, 0.0427, 0.0021, 0.0017, 0.0030, 0.0016, 0.0487,\n",
            "         0.0171, 0.0220, 0.0485, 0.0507, 0.0017, 0.0509, 0.0295, 0.0360, 0.0199,\n",
            "         0.0473, 0.0380, 0.0245, 0.0369, 0.0024, 0.0057, 0.0018, 0.0521, 0.0085,\n",
            "         0.0074]], grad_fn=<SoftmaxBackward0>)\n",
            "he transformer at last. i think i may have actuall make acage actually managed to make thing train! g\n",
            "memory: (tensor(-2.9552, grad_fn=<MinBackward1>), tensor(3.2677, grad_fn=<MaxBackward1>)) \n",
            " softmax: tensor([[0.0025, 0.0187, 0.0276, 0.0045, 0.0088, 0.0056, 0.0289, 0.0243, 0.0030,\n",
            "         0.0033, 0.0325, 0.0027, 0.0068, 0.0023, 0.0590, 0.0835, 0.0075, 0.0199,\n",
            "         0.0064, 0.0062, 0.0620, 0.0064, 0.0051, 0.0025, 0.0146, 0.0038, 0.0230,\n",
            "         0.0256, 0.0419, 0.0039, 0.0084, 0.0056, 0.0028, 0.0261, 0.0039, 0.0465,\n",
            "         0.0778, 0.0026, 0.0439, 0.1116, 0.0120, 0.0063, 0.0033, 0.0024, 0.0936,\n",
            "         0.0108]], grad_fn=<SoftmaxBackward0>)\n",
            "hk trans tring great news. now i can gh an gauanaged atually managed to make tcass. thin train! grato\n",
            "memory: (tensor(-3.9686, grad_fn=<MinBackward1>), tensor(4.0243, grad_fn=<MaxBackward1>)) \n",
            " softmax: tensor([[0.0655, 0.0200, 0.0122, 0.0021, 0.0668, 0.0029, 0.0693, 0.0032, 0.0547,\n",
            "         0.0231, 0.0018, 0.0020, 0.0036, 0.0116, 0.0666, 0.0043, 0.0066, 0.0023,\n",
            "         0.0036, 0.0024, 0.0026, 0.0034, 0.0457, 0.0020, 0.0028, 0.0204, 0.0070,\n",
            "         0.0079, 0.0226, 0.0194, 0.0024, 0.0567, 0.0597, 0.0057, 0.0237, 0.0278,\n",
            "         0.0724, 0.0578, 0.0110, 0.0612, 0.0128, 0.0359, 0.0031, 0.0065, 0.0020,\n",
            "         0.0029]], grad_fn=<SoftmaxBackward0>)\n",
            "he transformer at last. i think i may have actually managed to make this thing train! greact alusst.i\n",
            "memory: (tensor(-2.5002, grad_fn=<MinBackward1>), tensor(3.5508, grad_fn=<MaxBackward1>)) \n",
            " softmax: tensor([[0.0018, 0.0777, 0.0020, 0.0043, 0.0652, 0.0665, 0.0351, 0.0039, 0.0035,\n",
            "         0.0305, 0.0072, 0.0085, 0.0034, 0.0018, 0.0019, 0.0541, 0.0786, 0.0692,\n",
            "         0.0024, 0.0035, 0.0099, 0.0040, 0.0698, 0.0364, 0.0035, 0.0620, 0.0049,\n",
            "         0.0367, 0.0161, 0.0017, 0.0040, 0.0698, 0.0032, 0.0063, 0.0027, 0.0151,\n",
            "         0.0021, 0.0025, 0.0146, 0.0023, 0.0019, 0.0096, 0.0448, 0.0143, 0.0376,\n",
            "         0.0032]], grad_fn=<SoftmaxBackward0>)\n",
            "he transformer at last. i think i may have actually managed to make this thing train! great news. now\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-253-7f8d8af2df7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"h\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-243-1cd9a917b37e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, opt, dictionary, training_dataloader, test_data, length, start, forget)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;31m#backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpect_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-b2f1e8b4d1d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, activate, forget)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mactivate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-239-830c17289ec4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_control\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mread_output\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamplifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcelltoout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RvDH7SLyApfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}