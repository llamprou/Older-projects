{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from library_model import layers as lay\n",
    "from library_model import model_building as mb\n",
    "from data import data_loading as dt\n",
    "from data import text as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------\n",
    "#A CLASS TO CONTAIN THE NETWORK STATE\n",
    "#-----------------------------------------------------------------------------------\n",
    "class Network_state:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "    \n",
    "    class parameters:\n",
    "        ntokens = None\n",
    "        ntokens_out = None\n",
    "        d_model = None\n",
    "        nheads = None\n",
    "        d_key = None\n",
    "        d_hid = None\n",
    "        nlayers = None\n",
    "        attention_dropout =0.1\n",
    "        feedforward_dropout =0.\n",
    "        resnorm_dropout =0.1\n",
    "    \n",
    "    class training:\n",
    "        lr = None\n",
    "        batch_size = None\n",
    "        seq_length = None\n",
    "        optimizer = None\n",
    "        schedule = None\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#BUILD DECORATOR FOR RESIDUAL CONNECTION AND LAYER NORM\n",
    "#-----------------------------------------------------------------------------------\n",
    "#Skip connection and layer normalization decorator + dropout \n",
    "def SkipAndNormalize_decorator(cls):\n",
    "    class ResNorm_wrapper(nn.Module):\n",
    "        def __init__(self, hyper_param, size, layer_dropout, resnorm_dropout):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList([cls(hyper_param, layer_dropout), LayerNorm(size)])\n",
    "            self.dropout = nn.Dropout(resnorm_dropout)\n",
    "\n",
    "        \n",
    "        def forward(self, residual_stream, *input):\n",
    "            h = self.layers[0](*input) + residual_stream\n",
    "            return self.dropout(self.layers[1](h))\n",
    "    return ResNorm_wrapper    \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#BUILD DECORATORS FOR STACKS, EMBEDDING AND POSITIONAL ENCODING\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "#Stack decorator\n",
    "def get_stack(cls):\n",
    "    class Stack_wrapper(nn.Module):\n",
    "        def __init__(self, copies, skip_attention, skip_feedforward):\n",
    "            super().__init__()\n",
    "            blocks = [[copy.deepcopy(skip_attention), copy.deepcopy(skip_feedforward)] for _ in range(copies)]\n",
    "            self.layers = nn.ModuleList([cls(*block) for block in blocks])\n",
    "            self.external_input= None\n",
    "\n",
    "        def forward(self, input, *masks): #there can be 1 or 2 masks depending on whether we are stacking encoders or decoders\n",
    "            output = input\n",
    "            for layer in self.layers:\n",
    "                layer.encoder_output = self.external_input\n",
    "                output = layer(output, *masks)\n",
    "                layer.encoder_output = None\n",
    "            return output\n",
    "    return Stack_wrapper\n",
    "\n",
    "\n",
    "#define embedding and position encoding decorator\n",
    "def EmbedPosEncode(cls):\n",
    "    class EmbedPosEncodeWrapper(nn.Module):\n",
    "        def __init__(self, ntokens, d_model, nlayers, *blocks):\n",
    "            nn.Module.__init__(self)\n",
    "            self.cls = cls(nlayers, *blocks)\n",
    "            self.pos_enc = Positional_enc(d_model, max_dim= 5000)\n",
    "            self.embed=nn.Embedding(ntokens, d_model)\n",
    "            self.external_input = None\n",
    "        \n",
    "        def forward(self, input, *masks):\n",
    "            h = self.embed(input)\n",
    "            h = self.pos_enc(math.sqrt(h.size(-1))*h)\n",
    "            self.cls.external_input = self.external_input\n",
    "            output = self.cls(h, *masks)\n",
    "            self.cls.external_input = None\n",
    "            return output\n",
    "    return EmbedPosEncodeWrapper\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#BUILD THE BASIC LAYERS\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Positional encoding class\n",
    "class Positional_enc(nn.Module):\n",
    "    def __init__(self, dim_in, max_dim=5000):\n",
    "        nn.Module.__init__(self)\n",
    "        self.dim_in, self.max_dim = dim_in, max_dim\n",
    "        #construct positional encoding for single batch element\n",
    "        argument = torch.tensordot(torch.arange(max_dim, dtype=torch.float), torch.exp(-math.log(10000) *torch.arange(0, dim_in, 2, dtype=torch.float)/dim_in) , dims= 0)\n",
    "        pos_enc= torch.empty(max_dim, dim_in)\n",
    "        pos_enc[:, 0::2] = torch.sin(argument)\n",
    "        pos_enc[:, 1::2] = torch.cos(argument)\n",
    "        #introduce batch dimension (=0) \n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "        self.register_buffer(\"pos_enc\", pos_enc)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input =input + self.pos_enc[:, :input.size(1), :].requires_grad_(False)\n",
    "        return self.dropout(input)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#Mask\n",
    "def construct_mask(size, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    uppertri = torch.triu(torch.ones(1, size, size), diagonal=1)\n",
    "    return (uppertri ==0).to(device)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#define linear layer class\n",
    "class linear_layer(nn.Module):\n",
    "    def __init__(self, hyper_param): #(inp_dim, hid_dim, bias_is_true, relu_is_true)\n",
    "        super().__init__()\n",
    "        (self.inp_dim, self.hid_dim, self.bias_is_true, self.relu_is_true) = hyper_param\n",
    "        self.weight = nn.Parameter(torch.randn(self.inp_dim, self.hid_dim)/torch.sqrt(torch.tensor(self.inp_dim)))\n",
    "        if self.bias_is_true:\n",
    "            self.bias = nn.Parameter(torch.randn(self.hid_dim))\n",
    "        self.relu =nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output= torch.tensordot(input, self.weight,  dims = ([-1],[0]) ) \n",
    "        if self.bias_is_true:\n",
    "            output+= self.bias\n",
    "        if self.relu_is_true:\n",
    "            output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "#define layer norm class\n",
    "#Both the Annotated transformer and pytorch use 2 extra learnable parameters in this layer so I include them\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.epsilon= 10**(-7)\n",
    "        self.norm1 = nn.Parameter(torch.ones(size))\n",
    "        self.norm2 = nn.Parameter(torch.zeros(size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        mean = torch.mean(input, dim= -1, keepdim= True)\n",
    "        std = torch.std(input, dim=-1, keepdim= True) + self.epsilon\n",
    "        output = self.norm1*(input - mean)*(1/std) + self.norm2\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "#Convolutional-Pooling class\n",
    "class ConvPool(nn.Module):\n",
    "    def __init__(self, filter_dim, in_features, out_features, pooling=2.): #stride is set tp 1 and no padding\n",
    "        super().__init__()\n",
    "        self.filter_dim, self.indim, self.outdim, self.pool= filter_dim, in_features, out_features, pooling\n",
    "        self.filter = linear_layer((in_features*(filter_dim**2), out_features, True, False))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def convolution(self, input, dim0, dim1): #input: (batch_dim, feature_dim, image_dim0*image_dim1), dims = (image_dim0, image_dim1)\n",
    "        x = input.transpose(-1,-2).reshape(-1, dim0, dim1, self.indim)\n",
    "        w = self.filter_dim\n",
    "        output = torch.empty(x.size(0), (dim0-w +1), (dim1-w+1), self.indim*(w**2)).to(x)\n",
    "        \n",
    "        #A single small for loop over the size of the filter_dim/stride\n",
    "        for i in range(w):\n",
    "            for j in range(w):\n",
    "                y = x[:,i:,j:] #make a step equal to the stride=1\n",
    "                y = y[:,:(y.size(1)//w)*w, :(y.size(2)//w)*w] #truncate to right size\n",
    "                output[:, i::w, j::w,:]= y.view(-1, y.size(1)//w, w, y.size(2)//w, w, self.indim).transpose(-3,-4).reshape(-1, y.size(1)//w, y.size(2)//w, self.indim*(w**2)) \n",
    "        return self.filter(output).view(output.size(0), -1, self.outdim).transpose(-1,-2)\n",
    "\n",
    "    def pooling(self, input, dim0, dim1):\n",
    "        w = int(self.pool)\n",
    "        x = (input.view(-1, self.outdim, dim0, dim1))[:,:, :int((dim0//w)*w), :int((dim1//w)*w)] #pad down to appopriate size\n",
    "        x = x.view(-1, self.outdim, dim0//w, w, dim1//w, w).transpose(-2,-3).reshape(-1, self.outdim, dim0//w, dim1//w, w**2)\n",
    "        x_pool, _ = torch.max(x, dim=-1)\n",
    "        return x_pool\n",
    "\n",
    "\n",
    "    def forward(self, input, dim0, dim1): #flattened input\n",
    "        out = self.convolution(input, dim0, dim1)\n",
    "        new_dim0, new_dim1 = (dim0-self.filter_dim +1), (dim1-self.filter_dim +1)\n",
    "        if self.pool>1.:\n",
    "            out = self.pooling(out, new_dim0, new_dim1)\n",
    "        return out.view(out.size(0), out.size(1),-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#define FeedForward class\n",
    "@SkipAndNormalize_decorator\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hyper_param, dropout=0.): #([(dim_in, dim_out, bias_is_true, relu_is_true) ,(), ... ])\n",
    "        nn.Module.__init__(self) \n",
    "        self.hyper_param = hyper_param\n",
    "        self.layers = nn.ModuleList([linear_layer(param) for param in self.hyper_param])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return self.dropout(output)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "#multiheaded attention class\n",
    "@SkipAndNormalize_decorator\n",
    "class attention(nn.Module):\n",
    "    def __init__(self, hyper_param, dropout=0.1): #(dim_in, dim_key, dim_heads)\n",
    "        nn.Module.__init__(self) \n",
    "        (self.dim_in, self.dim_key, self.heads) = hyper_param\n",
    "        self.attention = nn.ModuleList([linear_layer((self.dim_in, self.dim_key*self.heads, False, False)) for _ in range(3)])\n",
    "        self.final = linear_layer((self.heads * self.dim_key, self.dim_in, False, False))\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    #Pytorch combines q,k,v into a sinlge large tensor and separates them for computation using tensor views\n",
    "    def forward(self, inputs, mask=None):\n",
    "        q,k,v = tuple(layer(inp).view(inp.size(0), inp.size(1), self.heads, self.dim_key).transpose(1,2) for inp, layer in zip(inputs, self.attention))\n",
    "        score = torch.matmul(q,k.transpose(-1,-2)).masked_fill(mask==0, -1e9) if mask is not None else torch.matmul(q,k.transpose(-1,-2))\n",
    "        p_atten = self.dropout(self.softmax(score/torch.sqrt(torch.tensor(self.dim_key))))\n",
    "        preactivation = torch.matmul(p_atten,v).transpose(1,2).reshape(v.size(0), -1, self.heads*self.dim_key)\n",
    "        return self.final(preactivation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#define encoder class\n",
    "@EmbedPosEncode\n",
    "@get_stack\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, skip_attention, skip_feedforward):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([skip_attention, skip_feedforward])\n",
    "\n",
    "    \n",
    "    def forward(self, input, mask=None): \n",
    "        inputs = [input for _ in range(3)]\n",
    "        h= self.layers[0](input, inputs, mask)\n",
    "        output = self.layers[1](h, h)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# define decoder class\n",
    "@EmbedPosEncode\n",
    "@get_stack\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, skip_attention, skip_feedforward):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([skip_attention, copy.deepcopy(skip_attention), skip_feedforward ])\n",
    "        self.encoder_output = None\n",
    "\n",
    "    def forward(self, input, enc_mask=None, dec_mask=None): #input = list [encoder_output, decoder_input]\n",
    "        h1 = self.layers[0](input, [input for _ in range(3)], dec_mask)\n",
    "        h2 = self.layers[1](h1, [h1, self.encoder_output, self.encoder_output], enc_mask)\n",
    "        output = self.layers[2](h2, h2)\n",
    "        return output\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#BUILD TRANSFORMER MODELS\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "#Encoder model class\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, encoder, linear):\n",
    "        nn.Module.__init__(self)\n",
    "        self.encoder = encoder\n",
    "        self.linear = linear\n",
    "  \n",
    "    def forward(self, input):\n",
    "        h = self.encoder(input, construct_mask(input.size(1)))\n",
    "        return self.linear(h)\n",
    "\n",
    "\n",
    "#Encoder-Decoder Transformer class\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, linear):\n",
    "        nn.Module.__init__(self)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.linear = linear\n",
    "  \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        h = self.encoder(encoder_input, enc_mask = None)\n",
    "        self.decoder.external_input = h\n",
    "        out = self.decoder(decoder_input, enc_mask = None, dec_mask = construct_mask(decoder_input.size(1)))\n",
    "        self.decoder.external_input = None\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "#GET TRANSFORMER BUILDING BLOCKS\n",
    "#-----------------------------------------------------------------------------------\n",
    "def get_circuits(state): #state is a class containing the NN and data hyperparameters\n",
    "    p=state.parameters\n",
    "    att_hyperparams = (p.d_model, p.d_key, p.nheads)\n",
    "    ff_hyperparams = [(p.d_model, p.d_hid, True, True), (p.d_hid, p.d_model, True, False)]\n",
    "    return lay.attention(att_hyperparams, p.d_model, p.attention_dropout, p.resnorm_dropout).to(state.device), lay.FeedForward(ff_hyperparams, p.d_model, p.feedforward_dropout, p.resnorm_dropout).to(state.device)\n",
    "         \n",
    "\n",
    "def get_transformer_parts(state): #state is a class containing the NN and data hyperparameters\n",
    "    p= state.parameters\n",
    "    att, ff = get_circuits(state)\n",
    "    \n",
    "    class TransformerParts:\n",
    "        attention = att\n",
    "        feedforward = ff\n",
    "        encoder = lay.Encoder(p.ntokens, p.d_model, p.nlayers, attention, feedforward).to(state.device)\n",
    "        decoder = lay.Decoder(p.ntokens, p.d_model, p.nlayers, attention, feedforward).to(state.device)\n",
    "        linear = lay.linear_layer((p.d_model, p.ntokens_out, False, False)).to(state.device)\n",
    "    return TransformerParts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = Network_state()\n",
    "p= state.parameters\n",
    "tr= state.training\n",
    "\n",
    "p.d_model=10\n",
    "p.d_hid=10\n",
    "p.nheads=2\n",
    "p.d_key = p.d_model //p.nheads\n",
    "p.nlayers=2\n",
    "p.ntokens=p.ntokens_out =300\n",
    "\n",
    "tr.batch_size=20\n",
    "tr.seq_length=35\n",
    "tr.optimizer = \"sgd\"\n",
    "tr.schedule = mb.learning_rate_step(5., 20/19, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = get_transformer_parts(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f1af38ef70d2ba974426d7a8a6b596b940002c76a66dd7cd7f620996d04d624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
